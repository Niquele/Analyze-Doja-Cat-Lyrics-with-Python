{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41a291e1",
   "metadata": {},
   "source": [
    "Analizando Doja Cat Lyrics com Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9efe657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1cb769",
   "metadata": {},
   "source": [
    "Explorando o Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "31821717",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Doja_Cat_lyrics_all.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [75]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m all_lyrics \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDoja_Cat_lyrics_all.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Doja_Cat_lyrics_all.csv'"
     ]
    }
   ],
   "source": [
    "all_lyrics = pd.read_csv('Doja_Cat_lyrics_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec18235",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lyrics.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb45776",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lyrics.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637a273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lyrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa835c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lyrics.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee64921",
   "metadata": {},
   "source": [
    "Limpando o Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b61365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alterando Lyrics para minúscula\n",
    "all_lyrics['new_lyrics'] = all_lyrics['Lyric'].str.lower()\n",
    "#Removendo pontuações\n",
    "all_lyrics['new_lyrics']= all_lyrics['new_lyrics'].str.replace('[^\\w\\s]',' ')\n",
    "#Removendo Stopwords\n",
    "stop = ['is','it','oh','of','uh','that','this','yeah','le','might',\n",
    "        'a','am', 'was', 'were', 'been','la','hey','b', 'n','is','to','end','in','the','t']\n",
    "\n",
    "all_lyrics['new_lyrics'] = all_lyrics['new_lyrics'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4d3eded7",
   "metadata": {},
   "source": [
    "Criando lista de palavras-chaves, realizando contagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec54745",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_affirmation = ['love','friend','friendship','great','proud','baby','mine','feel','feelings','perfect',\n",
    "                     'precious','like you','beauty']\n",
    "quality_time = ['watch','never','now','time','spend','text','texting','follow','chill','alone','play',\n",
    "                   'stay','party','talking','talk','sleep']\n",
    "physical_touch = ['kiss','love','sex','fuck','eyes','fight','hold','sexuality','face','body','mouth','dick',\n",
    "                'ass','touch','eye','pussy','eye']\n",
    "acts_service = ['turn','clean','stay','act','patient','relax','drive','notice','work','need','wait','focused',\n",
    "              'influence', 'struggle','sleep','share','sharing','honest']\n",
    "receiving_gifts = ['weed','sweet','ticket','bussines','send','spent','t-shirt','tiffanys','camera','getting','get',\n",
    "            'money','cash','more','ring','picture','clothes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d0d49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando uma categoria para cada lista de palavra\n",
    "words_affirmation_regex = '|'.join(words_affirmation)\n",
    "quality_time_regex = '|'.join(quality_time)\n",
    "physical_touch_regex = '|'.join(physical_touch)\n",
    "acts_service_regex = '|'.join(acts_service)\n",
    "receiving_gifts_regex = '|'.join(receiving_gifts)\n",
    "\n",
    "#Nova coluna para cada categoria\n",
    "all_lyrics['Words of Affirmation'] = all_lyrics['new_lyrics'].str.contains(words_affirmation_regex)\n",
    "all_lyrics['Quality Time'] = all_lyrics['new_lyrics'].str.contains(quality_time_regex)\n",
    "all_lyrics['Physical Touch'] = all_lyrics['new_lyrics'].str.contains(physical_touch_regex)\n",
    "all_lyrics['Acts of Service'] = all_lyrics['new_lyrics'].str.contains(acts_service_regex)\n",
    "all_lyrics['Receiving Gifts'] = all_lyrics['new_lyrics'].str.contains(receiving_gifts_regex)\n",
    "\n",
    "#Contando e realizando soma de quantidade de vezes em que as palavras da categoria são mencionadas \n",
    "words_affirmation_count = sum(all_lyrics['Words of Affirmation'])\n",
    "quality_time_count = sum(all_lyrics['Quality Time'])\n",
    "physical_touch_count = sum(all_lyrics['Physical Touch'])\n",
    "acts_service_count = sum(all_lyrics['Acts of Service'])\n",
    "receiving_gifts_count = sum(all_lyrics['Receiving Gifts'])\n",
    "\n",
    "#print\n",
    "print(\"Words of Affirmation: \", words_affirmation_count)\n",
    "print(\"Quality Time: \", quality_time_count)\n",
    "print(\"Physical Touch: \", physical_touch_count)\n",
    "print(\"Acts of Service: \", acts_service_count)\n",
    "print(\"Receiving Gifts: \", receiving_gifts_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50932ae",
   "metadata": {},
   "source": [
    "Selecionando as duas maiores linguagens para vizualisar as menções através dos a albums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eab0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Novo dataframe agrupado pelo album\n",
    "mentions = all_lyrics.groupby('Album Name').sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07efbff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions.sort_values(by='Words of Affirmation', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb21b135",
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions.sort_values(by='Quality Time', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f110a7e",
   "metadata": {},
   "source": [
    "Entendendo o foco dos lyrics da Doja Cat através do tokenice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3f1a01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Criando token das palavras\n",
    "all_lyrics['lyrics_tok'] = all_lyrics['new_lyrics'].apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd89c4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determinando palavras frequentemente usadas e criando nova lista.\n",
    "word_list = [word for list_ in all_lyrics['lyrics_tok'] for word in list_]\n",
    "#Função de contagem\n",
    "word_frequency = collections.Counter(word_list)\n",
    "#Organizando a frequência\n",
    "word_frequency = sorted(word_frequency.items(), key=lambda x: x[1], reverse=True)\n",
    "word_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3504d80c",
   "metadata": {},
   "source": [
    "Entendendo sentimentos dos Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0259c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função para análise de sentimentos\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "#Teste\n",
    "sia.polarity_scores(\"I like Doja Cat!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d4c1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando nova coluna e aplicando sia com a expressão lambda\n",
    "all_lyrics['polarity'] = all_lyrics['new_lyrics'].apply(lambda x: sia.polarity_scores(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d612f8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lyrics[['neg', 'neu', 'pos', 'compound']] = all_lyrics['polarity'].apply(pd.Series)\n",
    "#Excluindo coluna\n",
    "all_lyrics.drop('polarity', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b7694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculando sentimento\n",
    "pos = sum(all_lyrics['pos'])\n",
    "neu = sum(all_lyrics['neu'])\n",
    "neg = sum(all_lyrics['neg'])\n",
    "compound = sum(all_lyrics['compound'])\n",
    "\n",
    "#Print\n",
    "print(\"positive: \", pos)\n",
    "print(\"neutral: \", neu)\n",
    "print(\"negative: \", neg)\n",
    "print(\"compound: \", compound)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d0fe90",
   "metadata": {},
   "source": [
    "Descobrindo os sentimentos referentes a cada linguagem do amor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9031c01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando dataframes\n",
    "words_affirmation = all_lyrics[all_lyrics['Words of Affirmation']==True]\n",
    "quality_time = all_lyrics[all_lyrics['Quality Time']==True]\n",
    "physical_touch = all_lyrics[all_lyrics['Physical Touch']==True]\n",
    "acts_service = all_lyrics[all_lyrics['Acts of Service']==True]\n",
    "receiving_gifts = all_lyrics[all_lyrics['Receiving Gifts']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f1625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculando sentimentos de cada dataframe utilizando o compound\n",
    "words_affirmation_sentiment = words_affirmation['compound'].sum()\n",
    "quality_time_sentiment = quality_time['compound'].sum()\n",
    "physical_touch_sentiment = physical_touch['compound'].sum()\n",
    "acts_service_sentiment =acts_service['compound'].sum()\n",
    "receiving_gifts_sentiment = receiving_gifts['compound'].sum()\n",
    "\n",
    "#Print\n",
    "print(\"Words of Affirmation: \", words_affirmation_sentiment)\n",
    "print(\"Quality Time: \", quality_time_sentiment)\n",
    "print(\"Physical Touch: \", physical_touch_sentiment)\n",
    "print(\"Acts of Service: \", acts_service_sentiment)\n",
    "print(\"Receiving Gifts: \", receiving_gifts_sentiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
